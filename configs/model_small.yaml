model:
  embedding_dim: 256  # Even smaller for testing
  num_heads: 2
  num_layers: 2
  mlp_hidden_dim: 1024  # 4x embedding dim
  tree_embedding_dim: 256
  max_sequence_length: 1024

training:
  batch_size: 2
  learning_rate: 1e-3
  # warmup_steps: 100
  grad_clip: 1.0
  optimizer:
    name: "adam"  # Can be "adam", "adamw", "sgd", or "adafactor"
    memory_efficient: true  # Use fused implementation when available
  # scheduler:
  #   name: "cosine_warmup"  # Can be "cosine_warmup", "linear_warmup", "constant", or "none"
  #   warmup_steps: 1000  # Moving warmup_steps here for better organization
  mixed_precision: false
  gradient_checkpointing: false
  gradient_accumulation_steps: 32
  save_interval: 1