This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-20T23:16:45.469Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
configs/
  model_large.yaml
  model_small.yaml
.gitignore
.python-version
constants.py
count_params.py
data.py
layers.py
pyproject.toml
README.md
tokenizer.py
train.py
utils.py

================================================================
Files
================================================================

================
File: configs/model_large.yaml
================
# Large model (~200M parameters) for production
model:
  embedding_dim: 768  # GPT2-small size
  num_heads: 12
  num_layers: 12
  mlp_hidden_dim: 3072  # 4x embedding dim
  tree_embedding_dim: 768
  max_sequence_length: 1024
  vocab_size: 260  # Unchanged - depends on tokenizer

training:
  batch_size: 32
  learning_rate: 1e-4
  warmup_steps: 1000
  grad_clip: 1.0

================
File: configs/model_small.yaml
================
model:
  embedding_dim: 256  # Even smaller for testing
  num_heads: 2
  num_layers: 2
  mlp_hidden_dim: 1024  # 4x embedding dim
  tree_embedding_dim: 256
  max_sequence_length: 1024

training:
  batch_size: 1
  learning_rate: 1e-4
  warmup_steps: 1000
  grad_clip: 1.0
  optimizer:
    name: "sgd"  # Options: adamw, adam, sgd, adafactor
    memory_efficient: true
  mixed_precision: true  # Enable automatic mixed precision
  gradient_checkpointing: true  # Enable gradient checkpointing

================
File: .gitignore
================
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv

================
File: .python-version
================
3.12

================
File: constants.py
================
# Constants
MAX_TAXA = 16  # Maximum number of taxa (0-255)
VOCAB_SIZE = MAX_TAXA + 3  # 256 taxa + 2 special tokens (INTERNAL_NODE, EOS)
EMBEDDING_DIM = 768  # Following GPT2-small
NUM_HEADS = 12  # Following GPT2-small
NUM_LAYERS = 12  # Following GPT2-small
MLP_HIDDEN_DIM = 3072  # Following GPT2-small (4x embedding dim)
TREE_EMBEDDING_DIM = 768  # Same as model dimension for simplicity
MAX_SEQUENCE_LENGTH = 1024  # Following GPT2-small
MAX_GTREES = 300  # or a smaller/larger limit you know won't exceed the actual data

# Special tokens (match tokenizer values)
INTERNAL_NODE = VOCAB_SIZE - 2  # Internal node token
EOS = VOCAB_SIZE - 1  # End of sequence token
PAD = VOCAB_SIZE - 3  # Padding token

================
File: count_params.py
================
import argparse
import torch
from layers import TreeTransformer, ModelConfig

def count_parameters(model: torch.nn.Module) -> tuple[int, int]:
    """Count total and trainable parameters in the model"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to model config YAML")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = ModelConfig.from_yaml(args.config)
    model = TreeTransformer(config).to(device)
    total_params, trainable_params = count_parameters(model)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

================
File: data.py
================
import json
from dataclasses import dataclass
from typing import List, Tuple
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
import treeswift as ts
from rich.progress import track
from rich.console import Console
from rich import print as rprint
import argparse
import pyarrow.parquet as pq
import random
from multiprocessing import Pool, cpu_count
from constants import MAX_TAXA, MAX_GTREES, PAD
from tokenizer import NewickTokenizer

console = Console()

@dataclass
class InputPair:
    gtrees: list[str]
    stree: str

    @staticmethod
    def newick_to_distance_matrix(newick_str: str) -> np.ndarray:
        """
        Convert a single Newick format tree string to a topological distance matrix.
        Each entry d[i,j] represents the number of edges between taxa i and j,
        ignoring edge lengths.
        """
        tree = ts.read_tree_newick(newick_str)
        
        # Set all edge lengths to 1 for topological distance
        for node in tree.traverse_postorder():
            if not node.is_root():
                node.edge_length = 1
                
        # Get distance matrix as dictionary using TreeSwift's built-in method
        dist_dict = tree.distance_matrix(leaf_labels=True)
        
        # Get sorted list of taxa names to ensure consistent ordering
        taxa = sorted(dist_dict.keys())
        n = MAX_TAXA
        
        # Create numpy array from distance dictionary
        dist_matrix = np.zeros((n, n), dtype=np.uint8)
        for i, u in enumerate(taxa):
            for j, v in enumerate(taxa):
                if i != j:
                    dist_matrix[i,j] = int(dist_dict[u][v])
        return dist_matrix


def encode_distance_matrix(distance_matrix: np.ndarray) -> torch.Tensor:
    """
    Encode the upper triangular part of a distance matrix (excluding diagonal) into binary representation.
    """
    N = distance_matrix.shape[0]
    assert distance_matrix.shape == (N, N), "Input must be a square matrix"

    mask = torch.triu(torch.ones(N, N), diagonal=1).bool()
    distances = torch.from_numpy(distance_matrix).to(torch.uint8)
    binary_encoding = torch.zeros(N, N, 8, dtype=torch.float32)

    for bit in range(8):
        bit_value = (distances & (1 << bit)) >> bit
        binary_encoding[:, :, bit] = bit_value.float()

    flat_encoding = binary_encoding[mask]
    return flat_encoding


class TreeDataset(Dataset):
    def __init__(self, data_source: str, max_sequence_length: int = 1024, 
                 split: str = 'train', val_ratio: float = 0.2, seed: int = 42,
                 num_workers: int = 4):
        """
        Initialize the dataset.
        
        Args:
            data_source: Path to either .jsonl or .parquet file
            max_sequence_length: Maximum sequence length for tokenization
            split: Either 'train' or 'val'
            val_ratio: Ratio of directories to use for validation
            seed: Random seed for reproducibility
            num_workers: Number of processes for parallel preprocessing
        """
        self.max_sequence_length = max_sequence_length
        self.data: List[InputPair] = []
        self.tokenizer = NewickTokenizer()
        self.cached_encodings = []
        self.num_workers = min(num_workers, cpu_count())

        random.seed(seed)
        
        if data_source.endswith('.parquet'):
            self._load_from_parquet(data_source, split, val_ratio)
        else:
            self._load_from_jsonl(data_source)
            
        console.print(f"Pre-encoding trees for {split} split using {self.num_workers} processes...")
        self._parallel_encode_trees()

    def _load_from_parquet(self, parquet_path: str, split: str, val_ratio: float):
        """Load data from parquet file with train/val split based on directories"""
        table = pq.read_table(parquet_path)
        df = table.to_pandas()
        
        # Get unique directories and check if we have enough for directory-based split
        all_dirs = sorted(df['directory'].unique())
        n_val = int(len(all_dirs) * val_ratio)
        
        if n_val >= 1:
            # Directory-based split if we have enough directories
            val_dirs = set(random.sample(all_dirs, n_val))
            
            # Filter based on split
            if split == 'train':
                df = df[~df['directory'].isin(val_dirs)]
            else:  # val
                df = df[df['directory'].isin(val_dirs)]
        else:
            # Fallback to random row-based split if too few directories
            all_indices = list(range(len(df)))
            n_val_samples = int(len(df) * val_ratio)
            val_indices = set(random.sample(all_indices, n_val_samples))
            
            if split == 'train':
                df = df[~df.index.isin(val_indices)]
            else:  # val
                df = df[df.index.isin(val_indices)]
        
        console.print(f"Loading {split} split with {len(df)} examples")
        
        for _, row in df.iterrows():
            self.data.append(
                InputPair(gtrees=row['gtrees'], stree=row['species_tree'])
            )

    def _load_from_jsonl(self, jsonl_path: str):
        """Load data from jsonl file (legacy support)"""
        with open(jsonl_path, "r") as f:
            for line in f:
                item = json.loads(line)
                self.data.append(
                    InputPair(gtrees=item["gtrees"], stree=item["species_tree"])
                )

    def __len__(self) -> int:
        return len(self.data)

    def encode_trees(self, pair: InputPair) -> Tuple[torch.Tensor, torch.Tensor]:
        encoded_trees = []
        for tree in pair.gtrees:
            distance_matrix = InputPair.newick_to_distance_matrix(tree)
            encoded = encode_distance_matrix(distance_matrix)
            encoded_trees.append(encoded)

        tree_tensor = torch.stack(encoded_trees, dim=0)
        species_tokens = torch.tensor(self.tokenizer.encode(pair.stree))
        species_tokens = torch.cat(
            [
                species_tokens,
                torch.tensor([self.tokenizer.EOS]),
            ]
        )

        return tree_tensor, species_tokens

    def _encode_single_item(self, pair: InputPair) -> Tuple[torch.Tensor, torch.Tensor]:
        """Encode a single item for parallel processing"""
        return self.encode_trees(pair)

    def _parallel_encode_trees(self):
        """Parallel processing of tree encoding using chunks"""
        CHUNK_SIZE = 100
        with Pool(processes=self.num_workers) as pool:
            total = len(self.data)
            chunks = [self.data[i:i + CHUNK_SIZE] for i in range(0, total, CHUNK_SIZE)]
            
            with console.status(f"[bold green]Processing trees...") as status:
                processed = 0
                for chunk_results in pool.imap(
                    self._encode_chunk, 
                    chunks,
                    chunksize=1  # Each "chunk" here is already a batch of items
                ):
                    self.cached_encodings.extend(chunk_results)
                    processed += len(chunk_results)
                    console.print(f"Processed {processed}/{total} trees")

    def _encode_chunk(self, pairs: List[InputPair]) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Encode a chunk of items"""
        return [self.encode_trees(pair) for pair in pairs]

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        tree_tensor, species_tokens = self.cached_encodings[idx]
        
        # Get actual number of trees
        num_gene_trees = min(tree_tensor.size(0), MAX_GTREES)
        
        # Create padded tensor directly with zeros
        padded_tree_tensor = torch.zeros(
            (MAX_GTREES, tree_tensor.size(1), tree_tensor.size(2)),
            dtype=torch.float32
        )
        
        # Copy actual tree encodings
        padded_tree_tensor[:num_gene_trees] = tree_tensor[:num_gene_trees]
        
        return padded_tree_tensor, species_tokens


def parse_args():
    parser = argparse.ArgumentParser(description='Tree Dataset Loader')
    parser.add_argument('data_path', type=str, help='Path to data file (.jsonl or .parquet)')
    parser.add_argument('--max-seq-length', type=int, default=1024,
                      help='Maximum sequence length for tokenization')
    parser.add_argument('--val-ratio', type=float, default=0.2,
                      help='Ratio of directories to use for validation (for parquet files)')
    parser.add_argument('--seed', type=int, default=42,
                      help='Random seed for reproducibility')
    parser.add_argument('--num-workers', type=int, default=4,
                      help='Number of processes for parallel preprocessing')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    # Create datasets
    if args.data_path.endswith('.parquet'):
        train_dataset = TreeDataset(
            args.data_path, 
            max_sequence_length=args.max_seq_length,
            split='train',
            val_ratio=args.val_ratio,
            seed=args.seed,
            num_workers=args.num_workers
        )
        val_dataset = TreeDataset(
            args.data_path, 
            max_sequence_length=args.max_seq_length,
            split='val',
            val_ratio=args.val_ratio,
            seed=args.seed,
            num_workers=args.num_workers
        )
        
        console.print(f"[green]Dataset loaded successfully!")
        console.print(f"Train size: {len(train_dataset)}")
        console.print(f"Val size: {len(val_dataset)}")
    else:
        # Legacy jsonl support
        dataset = TreeDataset(
            args.data_path, 
            args.max_seq_length,
            num_workers=args.num_workers
        )
        console.print(f"[green]Dataset loaded successfully!")
        console.print(f"Dataset size: {len(dataset)}")
    
    # Look at first few items
    console.print("\n[yellow]Sample items:[/yellow]")
    dataset_to_inspect = train_dataset if args.data_path.endswith('.parquet') else dataset
    
    for i in range(min(3, len(dataset_to_inspect))):
        tree_tensor, species_tokens = dataset_to_inspect[i]
        console.print(f"\n[cyan]Item {i}:[/cyan]")
        console.print(f"Tree tensor shape: {tree_tensor.shape}")
        console.print(f"Species tokens shape: {species_tokens.shape}")
        console.print(f"First few species tokens: {species_tokens[:10]}")
        console.print(f"Tree tensor: {tree_tensor[:, :10]}")
        # Add decoded species tree output
        decoded_stree = dataset_to_inspect.tokenizer.decode(species_tokens.tolist())
        console.print(f"Decoded species tree: {decoded_stree}")

================
File: layers.py
================
import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
import yaml
import einops


from constants import (
    MAX_TAXA,
    VOCAB_SIZE,
    EMBEDDING_DIM,
    NUM_HEADS,
    NUM_LAYERS,
    MLP_HIDDEN_DIM,
    TREE_EMBEDDING_DIM,
    MAX_SEQUENCE_LENGTH,
    INTERNAL_NODE,
    EOS,
    PAD,
)


class DistanceMatrixMLP(nn.Module):
    """
    Single hidden layer MLP with SiLU activation for processing distance matrix encodings.
    Input dimension is calculated as: 8 * (n*(n-1)/2) where n is number of taxa
    and 8 is the binary distance encoding dimension.
    """

    def __init__(self, num_taxa: int, hidden_dim: int, output_dim: int):
        """
        Args:
            num_taxa: Number of taxa in the tree
            hidden_dim: Size of the hidden layer
            output_dim: Number of output features
        """
        super().__init__()
        
        # Calculate input dimension based on number of taxa
        num_distances = (num_taxa * (num_taxa - 1)) // 2
        input_dim = 8 * num_distances  # 8 is binary distance encoding dimension
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the MLP.

        Args:
            x: Input tensor of shape (batch_size, input_dim)

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, output_dim)
        """
        # First layer with SiLU activation
        x = F.silu(self.fc1(x))

        # Output layer (no activation)
        x = self.fc2(x)

        return x


class PositionalEncoding(nn.Module):
    """
    Sinusoidal positional encoding for non-recurrent transformers.
    Only used for output sequence positions.
    """

    def __init__(self, d_model: int, max_len: int = MAX_SEQUENCE_LENGTH):
        super().__init__()

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)
        )
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor, start_pos: int = 0) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape [batch_size, seq_length, embedding_dim]
            start_pos: Starting position for the sequence
        """
        return x + self.pe[start_pos : start_pos + x.size(1)]


@dataclass
class ModelConfig:
    embedding_dim: int
    num_heads: int
    num_layers: int
    mlp_hidden_dim: int
    tree_embedding_dim: int
    max_sequence_length: int

    @classmethod
    def from_yaml(cls, yaml_path: str) -> "ModelConfig":
        with open(yaml_path) as f:
            config = yaml.safe_load(f)
        return cls(**config["model"])  # Only use the "model" section


class TreeTransformer(nn.Module):
    """
    Encoder-decoder transformer model for processing gene trees and generating species trees.
    """

    def __init__(self, config: ModelConfig):
        super().__init__()

        # Token embedding layer
        self.token_embedding = nn.Embedding(VOCAB_SIZE, config.embedding_dim)

        # Tree embedding MLP
        self.tree_embedding = DistanceMatrixMLP(
            num_taxa=MAX_TAXA,  # Use MAX_TAXA from constants
            hidden_dim=config.embedding_dim * 2,
            output_dim=config.embedding_dim,
        )

        # Positional encoding
        self.pos_encoding = PositionalEncoding(config.embedding_dim, config.max_sequence_length)

        # Encoder layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.embedding_dim,
            nhead=config.num_heads,
            dim_feedforward=config.mlp_hidden_dim,
            batch_first=True,
            norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)

        # Decoder layers
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=config.embedding_dim,
            nhead=config.num_heads,
            dim_feedforward=config.mlp_hidden_dim,
            batch_first=True,
            norm_first=True,
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_layers)

        # Output projection
        self.output_projection = nn.Linear(config.embedding_dim, VOCAB_SIZE)

        # Enable support for gradient checkpointing
        self.gradient_checkpointing = False

    def gradient_checkpointing_enable(self):
        """Enable gradient checkpointing for memory efficiency"""
        ...
        # self.gradient_checkpointing = True
        # self.encoder.enable_gradient_checkpointing()
        # self.decoder.enable_gradient_checkpointing()

    def forward(
        self,
        tree_encodings: torch.Tensor,  # [batch_size, num_gene_trees, num_distances, 8]
        output_tokens: torch.Tensor = None,  # [batch_size, seq_len]
        attention_mask: torch.Tensor = None,
    ) -> torch.Tensor:
        batch_size, num_gene_trees, num_distances, bits = tree_encodings.shape

        

        # Reshape tree encodings to process each gene tree through MLP
        tree_encodings = einops.rearrange(
            tree_encodings,
            'b g d bits -> (b g) (d bits)',
            bits=8
        )
        encoded_trees = self.tree_embedding(tree_encodings)

        breakpoint()
        
        # Reshape back for encoder input
        encoder_input = einops.rearrange(
            encoded_trees,
            '(b g) e -> b g e',
            b=batch_size,
            g=num_gene_trees
        )

        # Create padding mask based on zero vectors
        # True indicates positions that should be masked (padded)
        tree_padding_mask = (tree_encodings.abs().sum(dim=-1) == 0).view(batch_size, num_gene_trees)

        # Run Transformer encoder
        memory = self.encoder(
            encoder_input,
            src_key_padding_mask=tree_padding_mask
        )

        if output_tokens is not None:
            # Training mode
            token_embeddings = self.token_embedding(output_tokens)
            decoder_input = self.pos_encoding(token_embeddings)

            # Generate causal mask if not provided
            if attention_mask is None:
                seq_length = output_tokens.size(1)
                attention_mask = torch.triu(
                    torch.ones(seq_length, seq_length), diagonal=1
                ).bool()
                attention_mask = attention_mask.to(output_tokens.device)

            # Use gradient checkpointing if enabled
            if self.gradient_checkpointing and self.training:
                hidden_states = torch.utils.checkpoint.checkpoint(
                    self.decoder,
                    decoder_input,
                    memory,
                    attention_mask
                )
            else:
                hidden_states = self.decoder(
                    decoder_input,
                    memory,
                    attention_mask
                )
        else:
            # Inference mode - start with memory
            hidden_states = memory

        # Project to vocabulary
        logits = self.output_projection(hidden_states)

        return logits

================
File: pyproject.toml
================
[project]
name = "gformer"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "einops>=0.8.0",
    "filelock>=3.16.1",
    "numpy>=2.2.2",
    "pandas>=2.2.3",
    "pyarrow>=19.0.0",
    "pyyaml>=6.0.2",
    "smallperm>=0.1.12",
    "torch>=2.5.1",
    "tqdm>=4.67.1",
    "treeswift>=1.1.45",
    "typer>=0.15.1",
]

[tool.uv]
dev-dependencies = [
    "ruff>=0.9.2",
]

================
File: README.md
================
# **Refined Model Architecture Overview**

## **1. Introduction**

This model is designed for converting a set of “gene trees” (in Newick format) into a single “species tree” (also in Newick format). Each gene tree is an unrooted phylogenetic tree; the species tree is typically rooted. The process can be seen as an **encoder-decoder** transformation:

- **Encoder** consumes the set of gene trees (treated as a set, thus order-invariant).
- **Decoder** then predicts the tokens of the species tree, one at a time, using an autoregressive approach.

The goal is to capture high-level phylogenetic signals from multiple gene trees and produce a single species tree that explains these signals.

---

## **2. Model Inputs and Representation**

### **2.1 Gene Trees**

1. **Input Format**  
   We assume \( k \) unrooted gene trees, each given in Newick notation.  
   - Example gene tree:  
     ```
     (0, (1, 2))
     ```
   - Each gene tree has up to 256 possible taxa labels (0 through 255).

2. **Distance Matrix**  
   For each gene tree, we create an **all-pairs topological distance matrix**.  
   - **Topological distance** here is the number of edges on the path between two taxa (i.e., ignoring branch lengths).  
   - If a tree has \( N \) leaves, this yields an \( N \times N \) matrix.  
   - We often fix \( N = 16 \) (or a known maximum) to simplify zero-padding.

3. **Binary Encoding**  
   We flatten the upper-triangular part of the \( N \times N \) matrix (excluding the diagonal) and store each distance in **binary** using 8 bits. This yields a 3D tensor of shape:  
   \[
   (\text{num\_upper\_triangular\_entries}) \times 8
   \]
   or in code terms, something like \(\tfrac{N(N-1)}{2} \times 8\).  
   - Each bit is extracted, then transformed by a small activation (SiLU).  
   - This flattening results in a consistent-length representation for each gene tree.

### **2.2 Species Tree (Output)**

1. **Tokenizer**  
   We have a simple integer-based tokenizer for Newick strings:
   - **Taxa labels**: integers \([0..255]\).  
   - **Special tokens**:
     - 256 = “(” (LEFT_PAREN)
     - 257 = “)” (RIGHT_PAREN)
     - 258 = End-of-input (EOI)
     - 259 = End-of-sequence (EOS)
   - The final vocabulary size is 260.

2. **Autoregressive Generation**  
   The decoder produces the species tree tokens one at a time. At step \( t \), it can only see the tokens \(\leq t-1\) that it has already produced (or teacher-forced).

---

## **3. High-Level Architecture**

### **3.1 Overview: Encoder–Decoder**

The model follows a **Transformer** design:

1. **Encoder**  
   Processes all \( k \) gene trees, each turned into an embedding vector.  
   - **Set Input**: We treat the \( k \) gene tree embeddings as a set (hence no positional encoding).  
   - The encoder uses standard multi-head self-attention across these \( k \) embeddings.

2. **Decoder**  
   Predicts the species tree token-by-token.  
   - **Positional Encoding** is applied to the partial species tree tokens.  
   - **Causal Masking** ensures the decoder can’t see future tokens.  
   - The decoder also attends to the encoder’s outputs (cross-attention) to incorporate the gene tree information.

### **3.2 Detailed Components**

Below is a diagram of the major stages:

```
Gene Trees --> DistanceMatrixMLP --> TransformerEncoder --> *encoded memory*
                                                   \ 
                                                    -> TransformerDecoder -> Output Tokens
Species Tokens + Positional Encoding ------------/
```

1. **DistanceMatrixMLP**  
   - Input dimension:  
     \[
     \underbrace{\left(\frac{N(N-1)}{2}\right)}_{\text{upper-triangular pairs}} \times \underbrace{8}_{\text{bits}}
     \]
   - Simple feedforward of shape \(\text{[in_dim, hidden_dim, out_dim]}\), with SiLU activation in the hidden layer.  
   - Outputs a **single embedding vector** per gene tree.

2. **Transformer Encoder**  
   - Takes a batch of gene-tree embeddings (\( k \)-by-\(\text{embedding\_dim}\)) and processes them with multi-head attention.  
   - We do not apply any position-based ordering, because the set of gene trees is permutation-invariant.  
   - Standard pre-layer normalization is used for training stability.  
   - This yields a memory representation of shape \(\text{[batch, k, d\_model]}\).

3. **Transformer Decoder**  
   - Consumes partial species-tree tokens, each embedded to dimension \( d_{\text{model}} \) via `nn.Embedding`.  
   - A **sinusoidal positional encoding** is added to these embeddings (since sequences, not sets, must reflect order).  
   - Self-attention with a **causal mask** ensures token \( t \) can’t see beyond \( t-1 \).  
   - **Cross-attention** keys/values come from the encoder memory.  
   - Produces a hidden state for each species-tree token.

4. **Final Projection**  
   - A linear projection maps each decoder hidden state to \(\text{vocab\_size} = 260\) logits.  
   - Softmax over these logits yields the next token distribution.

---

## **4. Training**

### **4.1 Setup**

1. **Teacher Forcing**  
   During training, the decoder sees the ground-truth partial sequence at each timestep. We shift the target tokens by one, so the model predicts the next token from the current partial sequence.

2. **Loss Function**  
   - We use cross-entropy loss over the species tree tokens.  
   - Special or padding tokens can be ignored using PyTorch’s built-in ability to exclude specific indices (e.g., the EOI token).

3. **Hyperparameters**  
   - **Batch size**: e.g., 32 for the large model or 1 for a minimal test.  
   - **Learning rate**: typically in the \(1\mathrm{e}{-4}\) range (configurable).  
   - **Warmup steps**: e.g., 1000, with a cosine decay schedule afterward.  
   - **Gradient checkpointing**: can optionally be enabled to reduce memory usage at the cost of some recomputation.

### **4.2 Optimizers**  
Common optimizers are supported, including **Adam, AdamW, SGD**, and **Adafactor**. We often rely on memory-efficient “fused” variants if available.

### **4.3 Validation**  
For validation, we measure:
1. **Validation Loss** using cross-entropy on a held-out subset.  
2. **Token Accuracy** by comparing predicted tokens vs. reference.  

Because tree topologies can have multiple correct forms, a direct token-level comparison is only a rough approximation of correctness. More advanced tree-specific metrics could be substituted if desired.

---

## **5. Implementation Notes**

### **5.1 Parallel Preprocessing**  
- A custom `TreeDataset` uses parallel workers to:
  - Parse and tokenize gene trees.
  - Build and cache distance-matrix encodings.
  - Construct token sequences for the species tree.

### **5.2 Handling Variable Gene Tree Counts**  
- We zero-pad the dimension “number of gene trees” to a fixed maximum (e.g., `MAX_GTREES = 300`), so the encoder can process them in a single batch.  
- If a particular input has fewer than `MAX_GTREES`, we pad with zeros; if more, we can truncate.

### **5.3 Limitations / Assumptions**  
1. **Fixed Taxa Limit**: The code expects a fixed \( N \leq 256 \). Large expansions might require memory or architectural changes.  
2. **No explicit root modeling** in the gene trees. We rely on topological distance.  
3. **Set-based gene trees**: We do not impose any order or weighting among the gene trees.

---

## **6. Example Workflow**

1. **Data**: Provide a dataset of pairs \(\{\text{list of gene trees}, \text{species tree}\}\).  
2. **Preprocessing**:
   - Convert each gene tree into its binary-encoded distance matrix flattening.  
   - Tokenize the species tree into integer IDs.  
3. **Training**:
   - The encoder processes the gene-tree embeddings.  
   - The decoder is fed partial species-tree tokens plus a causal mask.  
   - The model outputs distributions over the next token until the entire species tree is generated.  
4. **Inference**:
   - Given new gene trees, do the same distance-matrix encoding.  
   - Feed them to the encoder, then **autoregressively** decode the species tree tokens.  
   - Stop when an EOS token is generated.

---

## **7. Configurations**

We supply two example YAML configuration files:

1. **`model_large.yaml`**  
   - ~200M parameters, with GPT2-small-like dimensions (embedding_dim=768, 12 heads, 12 layers, MLP=3072).  
   - Good for production usage.

2. **`model_small.yaml`**  
   - ~28M parameters, smaller test version (embedding_dim=256, only 2 layers, etc.).  
   - Typically used for debugging or quick experiments.

---

## **8. Summary**

This architecture leverages a **set-based Transformer encoder** on binary-encoded distance matrices from multiple gene trees, followed by a **causal Transformer decoder** to produce the final species tree tokens. The design is flexible, relatively straightforward to train, and can handle large numbers of taxa and gene trees (subject to memory constraints). It captures phylogenetic signals by converting each gene tree to a topological embedding, then synthesizes them in an autoregressive manner to predict the species tree.

Overall, the model aims to unify signals from multiple gene trees in a robust, end-to-end trainable neural framework, producing consistent and meaningful species-tree predictions.

---

**End of Document**

================
File: tokenizer.py
================
from typing import List, Union

from constants import INTERNAL_NODE, EOS, PAD, VOCAB_SIZE

class NewickTokenizer:
    """
    Grammar-based tokenizer for Newick tree strings with taxa labels 0-255.
    This tokenizer emits tokens for internal nodes and leaves following a pre-order traversal.
    """

    INTERNAL_NODE = INTERNAL_NODE
    EOS = EOS
    PAD = PAD  # Add PAD token constant

    def __init__(self):
        self.vocab_size = VOCAB_SIZE  # Use VOCAB_SIZE from constants.py

    def encode(self, newick_str: str) -> List[int]:
        """
        Encode a Newick tree string into a list of tokens using a grammar-based approach.

        Args:
            newick_str: A Newick format tree string (e.g., "(1,(2,3));")

        Returns:
            List of integer tokens
        """
        def parse_subtree(index: int) -> (List[int], int):
            tokens = []
            if newick_str[index] == "(":
                tokens.append(self.INTERNAL_NODE)
                index += 1
                left_tokens, index = parse_subtree(index)
                tokens.extend(left_tokens)
                index += 1  # Skip the comma
                right_tokens, index = parse_subtree(index)
                tokens.extend(right_tokens)
                index += 1  # Skip the closing parenthesis
            else:
                # Parse a leaf label
                number = ""
                while index < len(newick_str) and newick_str[index].isdigit():
                    number += newick_str[index]
                    index += 1
                tokens.append(int(number))
            return tokens, index

        tokens, index = parse_subtree(0)
        tokens.append(self.EOS)  # Add EOS token at the end
        return tokens

    def decode(self, tokens: List[int]) -> str:
        """
        Decode a list of tokens back into a Newick tree string.
        Ignores PAD tokens during decoding.

        Args:
            tokens: List of integer tokens

        Returns:
            Newick format tree string
        """
        def build_subtree(index: int) -> (str, int):
            # Skip any PAD tokens
            while index < len(tokens) and tokens[index] == self.PAD:
                index += 1
                
            if index >= len(tokens):
                return "", index
                
            if tokens[index] == self.INTERNAL_NODE:
                index += 1
                left_subtree, index = build_subtree(index)
                right_subtree, index = build_subtree(index)
                subtree = f"({left_subtree},{right_subtree})"
            else:
                # Leaf node
                subtree = str(tokens[index])
                index += 1
            return subtree, index

        tree, _ = build_subtree(0)
        return tree + ";"  # Append the Newick end character

    def encode_batch(self, newick_strings: List[str]) -> List[List[int]]:
        """
        Encode a batch of Newick strings.

        Args:
            newick_strings: List of Newick format tree strings

        Returns:
            List of lists of integer tokens
        """
        return [self.encode(s) for s in newick_strings]

    def decode_batch(self, token_batches: List[List[int]]) -> List[str]:
        """
        Decode a batch of token lists.

        Args:
            token_batches: List of lists of integer tokens

        Returns:
            List of Newick format tree strings
        """
        return [self.decode(tokens) for tokens in token_batches]


if __name__ == '__main__':
    # Example usage
    stree = "(((1,14),(9,6)),((((15,(0,10)),(5,4)),12),((((7,11),13),(2,8)),3)));"
    tokenizer = NewickTokenizer()
    print("Original tree:", stree)

    encoded = tokenizer.encode(stree)
    print("Encoded tokens:", encoded)

    decoded = tokenizer.decode(encoded)
    print("Decoded tree:", decoded)

================
File: train.py
================
import argparse
import logging
from dataclasses import dataclass

import numpy as np
import torch
import os
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import math
import yaml
import treeswift as ts
from torch.cuda.amp import autocast, GradScaler
try:
    from transformers.optimization import Adafactor
    HAVE_ADAFACTOR = True
except ImportError:
    HAVE_ADAFACTOR = False

from layers import TreeTransformer, ModelConfig
from constants import VOCAB_SIZE, EOS, PAD
from data import TreeDataset
from tokenizer import NewickTokenizer


def encode_distance_matrix(distance_matrix: np.ndarray) -> torch.Tensor:
    """
    Encode the upper triangular part of a distance matrix (excluding diagonal) into binary representation.

    Args:
        distance_matrix: NxN numpy array containing uint8 distances

    Returns:
        torch.Tensor: Binary encoding of shape (num_upper_elements, 8) containing the flattened upper triangular values
    """
    N = distance_matrix.shape[0]
    assert distance_matrix.shape == (N, N), "Input must be a square matrix"

    # Create a mask for upper triangular part (excluding diagonal)
    mask = torch.triu(torch.ones(N, N), diagonal=1).bool()

    # Convert to torch tensor if not already
    distances = torch.from_numpy(distance_matrix).to(torch.uint8)

    # Create binary encoding matrix
    binary_encoding = torch.zeros(N, N, 8, dtype=torch.float32)

    # Get binary representation for each bit position (0-7)
    for bit in range(8):
        bit_value = (distances & (1 << bit)) >> bit
        binary_encoding[:, :, bit] = bit_value.float()

    # Apply SiLU activation to the binary encodings
    binary_encoding = F.silu(binary_encoding)

    # Extract only the upper triangular elements
    flat_encoding = binary_encoding[mask]

    return flat_encoding


@dataclass
class InputPair:
    gtrees: list[str]
    species_tree: str

    @staticmethod
    def newick_to_distance_matrix(newick_str: str) -> np.ndarray:
        """
        Convert a single Newick format tree string to a topological distance matrix.
        Each entry d[i,j] represents the number of edges between taxa i and j,
        ignoring edge lengths.
        
        Args:
            newick_str: Tree in Newick format
            
        Returns:
            np.ndarray: NxN distance matrix where N is number of taxa
        """
        # Parse newick string into TreeSwift tree
        tree = ts.read_tree_newick(newick_str)
        
        # Set all edge lengths to 1 for topological distance
        for node in tree.traverse_postorder():
            if not node.is_root():
                node.edge_length = 1
                
        # Get distance matrix as dictionary using TreeSwift's built-in method
        dist_dict = tree.distance_matrix(leaf_labels=True)
        
        # Get sorted list of taxa names to ensure consistent ordering
        taxa = sorted(dist_dict.keys())
        n = len(taxa)
        
        # Create numpy array from distance dictionary
        dist_matrix = np.zeros((n, n), dtype=np.uint8)
        for i, u in enumerate(taxa):
            for j, v in enumerate(taxa):
                if i != j:
                    # No need to round since all distances will be integers
                    dist_matrix[i,j] = int(dist_dict[u][v])
                    
        return dist_matrix


def create_causal_mask(
    seq_length: int, num_trees: int, device: torch.device
) -> torch.Tensor:
    """Create causal attention mask that allows trees to attend to each other"""
    mask = torch.ones(seq_length, seq_length, dtype=torch.bool, device=device)

    # Allow trees to attend to each other
    mask[:num_trees, :num_trees] = False

    # Create causal mask for output sequence
    mask[num_trees:, num_trees:] = torch.triu(
        torch.ones(
            seq_length - num_trees,
            seq_length - num_trees,
            dtype=torch.bool,
            device=device,
        ),
        diagonal=1,
    )

    return mask


def get_scheduler(optimizer, total_steps: int, warmup_steps: int):
    """
    Creates a learning rate scheduler with linear warmup and cosine decay
    """
    def lr_lambda(current_step: int):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
        
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def get_optimizer(model: torch.nn.Module, config: "TrainingConfig"):
    """Get memory efficient optimizer based on config"""
    optimizer_name = config.optimizer["name"].lower()
    
    if optimizer_name == "adamw":
        return torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            fused=config.optimizer.get("memory_efficient", False)  # Use fused if available
        )
    elif optimizer_name == "adam":
        return torch.optim.Adam(
            model.parameters(),
            lr=config.learning_rate,
            fused=config.optimizer.get("memory_efficient", False)
        )
    elif optimizer_name == "sgd":
        return torch.optim.SGD(
            model.parameters(),
            lr=config.learning_rate
        )
    elif optimizer_name == "adafactor" and HAVE_ADAFACTOR:
        return Adafactor(
            model.parameters(),
            lr=config.learning_rate,
            scale_parameter=False,
            relative_step=False
        )
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")


def train_epoch(
    model: TreeTransformer,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: torch.optim.lr_scheduler.LambdaLR,
    device: torch.device,
    epoch: int,
    scaler: GradScaler = None,
    mixed_precision: bool = False,
    gradient_accumulation_steps: int = 1,
) -> float:
    model.train()
    total_loss = 0
    optimizer.zero_grad()

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch}")
    for batch_idx, (tree_encodings, target_seq) in enumerate(progress_bar):
        tree_encodings = tree_encodings.to(device)
        target_seq = target_seq.to(device)
        
        # Use automatic mixed precision if enabled
        with autocast(enabled=mixed_precision):
            # Forward pass
            logits = model(
                tree_encodings=tree_encodings,
                output_tokens=target_seq[:, :-1],
                attention_mask=None  # Will be auto-generated
            )

            # Print shapes to understand the parallel processing
            print("Input sequence shape:", target_seq[:, :-1].shape)
            print("Output logits shape:", logits.shape)
            print("Target sequence shape:", target_seq[:, 1:].shape)
            
            # First sequence in batch
            print("\nFirst sequence:")
            print("Input:", target_seq[0, :-1])  # Input sequence
            print("Target:", target_seq[0, 1:])  # Expected predictions

            # Compute loss
            loss = F.cross_entropy(
                logits.reshape(-1, VOCAB_SIZE),
                target_seq[:, 1:].reshape(-1),
                ignore_index=PAD,
            )
            # Scale loss by accumulation steps
            loss = loss / gradient_accumulation_steps

        # Backward pass with mixed precision support
        if mixed_precision and scaler is not None:
            scaler.scale(loss).backward()
        else:
            loss.backward()

        # Update weights if we've accumulated enough gradients
        if (batch_idx + 1) % gradient_accumulation_steps == 0:
            if mixed_precision and scaler is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            
            optimizer.zero_grad()
            scheduler.step()

        total_loss += loss.item() * gradient_accumulation_steps
        progress_bar.set_postfix({"loss": loss.item() * gradient_accumulation_steps})

    return total_loss / len(dataloader)


def compute_tree_accuracy(predicted_tokens: torch.Tensor, target_tokens: torch.Tensor) -> float:
    """Compute accuracy of tree topology prediction"""
    # Ignore padding tokens in comparison
    mask = target_tokens != EOS  # Updated from END_OF_INPUT
    correct = (predicted_tokens == target_tokens) & mask
    return correct.float().sum() / mask.sum()


def validate(model, val_loader, device) -> dict:
    model.eval()
    metrics = {
        'loss': 0.0,
        'accuracy': 0.0
    }
    
    with torch.no_grad():
        for tree_encodings, target_seq in val_loader:
            tree_encodings = tree_encodings.to(device)
            target_seq = target_seq.to(device)

            num_trees = tree_encodings.size(1)
            seq_length = target_seq.size(1)
            
            sequence_loss = 0
            sequence_correct = 0
            total_tokens = 0
            
            # Evaluate each position in sequence
            for pos in range(1, seq_length):
                partial_target = target_seq[:, :pos]
                attention_mask = torch.triu(torch.ones(pos, pos), diagonal=1).bool().to(device)


                logits = model(
                    tree_encodings=tree_encodings,
                    output_tokens=partial_target,
                    attention_mask=attention_mask,
                )

                print(logits.shape)

                print(F.softmax(logits, dim=-1))

                # Loss for next token prediction
                loss = F.cross_entropy(
                    logits[:, -1, :],
                    target_seq[:, pos],
                )
                sequence_loss += loss.item()
                
                # Accuracy for next token prediction
                pred = logits[:, -1, :].argmax(dim=-1)
                correct = (pred == target_seq[:, pos])
                sequence_correct += correct.sum().item()
                total_tokens += target_seq.size(0)

            metrics['loss'] += sequence_loss / (seq_length - 1)
            metrics['accuracy'] += sequence_correct / total_tokens
    
    # Average metrics
    for k in metrics:
        metrics[k] /= len(val_loader)
        
    return metrics


def count_parameters(model: torch.nn.Module) -> tuple[int, int]:
    """Count total and trainable parameters in the model"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


@dataclass
class TrainingConfig:
    batch_size: int
    learning_rate: float
    warmup_steps: int
    grad_clip: float
    optimizer: dict
    mixed_precision: bool
    gradient_checkpointing: bool
    gradient_accumulation_steps: int
    save_interval: int

    @classmethod
    def from_yaml(cls, yaml_path: str) -> "TrainingConfig":
        with open(yaml_path) as f:
            config = yaml.safe_load(f)
        
        training_config = config["training"]
        return cls(
            batch_size=int(training_config["batch_size"]),
            learning_rate=float(training_config["learning_rate"]),
            warmup_steps=int(training_config["warmup_steps"]),
            grad_clip=float(training_config["grad_clip"]),
            optimizer=training_config["optimizer"],
            mixed_precision=training_config.get("mixed_precision", False),
            gradient_checkpointing=training_config.get("gradient_checkpointing", False),
            gradient_accumulation_steps=int(training_config.get("gradient_accumulation_steps", 1)),
            save_interval=int(training_config.get("save_interval", 1))
        )


def show_example_completions(model, val_loader, device, tokenizer, num_examples=5) -> None:
    """Show example completions from the model"""
    model.eval()
    
    with torch.no_grad():
        # Get a batch of data
        tree_encodings, target_seq = next(iter(val_loader))
        tree_encodings = tree_encodings.to(device)
        target_seq = target_seq.to(device)
        
        # Show completions for first num_examples in batch
        for i in range(min(num_examples, target_seq.size(0))):
            # Get ground truth completion
            gt_completion = tokenizer.decode(target_seq[i].cpu().tolist())
            
            # Start with just first token (usually INTERNAL_NODE)
            curr_seq = target_seq[i:i+1, :1]  
            
            # Generate tokens until we hit EOS or max length
            for pos in range(target_seq.size(1) - 1):
                # Create causal mask for current sequence length
                attention_mask = torch.triu(
                    torch.ones(curr_seq.size(1), curr_seq.size(1)), 
                    diagonal=1
                ).bool().to(device)
                
                # Get next token prediction
                logits = model(
                    tree_encodings=tree_encodings[i:i+1],  # Keep batch dimension
                    output_tokens=curr_seq,
                    attention_mask=attention_mask
                )
                
                # Get most likely next token
                next_token = logits[:, -1:, :].argmax(dim=-1)
                
                # Break if we hit EOS
                if next_token.item() == tokenizer.EOS:
                    break
                    
                # Add predicted token to sequence
                curr_seq = torch.cat([curr_seq, next_token], dim=1)
            
            # Decode prediction
            pred_completion = tokenizer.decode(curr_seq[0].cpu().tolist())
            
            # Print comparison
            print(f"\nExample {i+1}:")
            print(f"Ground truth: {gt_completion}")
            print(f"Prediction:   {pred_completion}")
            
            # Print first few logits for debugging
            if pos == 0:
                probs = F.softmax(logits[0, 0], dim=-1)
                top_k = 5
                values, indices = probs.topk(top_k)
                print("\nTop predictions for first position:")
                for prob, idx in zip(values, indices):
                    if idx == tokenizer.INTERNAL_NODE:
                        token_name = "INTERNAL_NODE"
                    elif idx == tokenizer.EOS:
                        token_name = "EOS"
                    else:
                        token_name = str(idx.item())
                    print(f"{token_name}: {prob:.3f}")


def main():
    parser = argparse.ArgumentParser(description="Train TreeTransformer model")
    subparsers = parser.add_subparsers(dest='command', help='Command to run')
    
    # Training subcommand
    train_parser = subparsers.add_parser('train', help='Train the model')
    train_parser.add_argument(
        "--data", type=str, required=True, 
        help="Path to data file (.jsonl or .parquet)"
    )
    train_parser.add_argument(
        "--lr", type=float,
        help="Learning rate (overrides config file)"
    )
    train_parser.add_argument(
        "--val-ratio", type=float, default=0.2,
        help="Ratio of directories to use for validation (for parquet files)"
    )
    train_parser.add_argument(
        "--config", type=str, required=True, help="Path to model config YAML"
    )
    train_parser.add_argument(
        "--batch-size", type=int, 
        help="Batch size (overrides config file)"
    )
    train_parser.add_argument("--epochs", type=int, default=100, help="Number of epochs")
    train_parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to train on",
    )
    train_parser.add_argument(
        "--save-dir",
        type=str,
        default="checkpoints",
        help="Directory to save model checkpoints",
    )
    train_parser.add_argument(
        '--grad-clip', type=float,
        help='Gradient clipping value (overrides config file)'
    )
    train_parser.add_argument(
        '--warmup-steps', type=int,
        help='Learning rate warmup steps (overrides config file)'
    )
    train_parser.add_argument('--seed', type=int, default=42,
                       help='Random seed for reproducibility')
    train_parser.add_argument('--num-workers', type=int, default=4,
                       help='Number of processes for parallel preprocessing')
    train_parser.add_argument(
        '--gradient-accumulation-steps', 
        type=int,
        help='Number of gradient accumulation steps (overrides config file)'
    )
    train_parser.add_argument(
        '--save-interval',
        type=int,
        help='Save checkpoint every N epochs (overrides config file)'
    )

    # Count parameters subcommand
    count_parser = subparsers.add_parser('count-params', help='Count model parameters')
    count_parser.add_argument(
        "--config", type=str, required=True, help="Path to model config YAML"
    )
    count_parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to initialize model on",
    )

    args = parser.parse_args()

    if args.command == 'count-params':
        device = torch.device(args.device)
        config = ModelConfig.from_yaml(args.config)
        model = TreeTransformer(config).to(device)
        total_params, trainable_params = count_parameters(model)
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        return

    elif args.command == 'train':
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        # Create save directory
        os.makedirs(args.save_dir, exist_ok=True)

        # Initialize model and move to device
        device = torch.device(args.device)
        config = ModelConfig.from_yaml(args.config)
        model = TreeTransformer(config).to(device)

        # Create datasets with automatic train/val splitting
        train_dataset = TreeDataset(
            args.data,
            split='train',
            val_ratio=args.val_ratio,
            seed=args.seed,
            num_workers=args.num_workers
        )
        
        val_dataset = TreeDataset(
            args.data,
            split='val', 
            val_ratio=args.val_ratio,
            seed=args.seed,
            num_workers=args.num_workers
        )

        logger.info(f"Training dataset size: {len(train_dataset)}")
        logger.info(f"Validation dataset size: {len(val_dataset)}")

        # Load both configs
        training_config = TrainingConfig.from_yaml(args.config)

        # Use config values unless overridden by command line
        batch_size = args.batch_size or training_config.batch_size
        learning_rate = args.lr or training_config.learning_rate
        warmup_steps = args.warmup_steps or training_config.warmup_steps
        grad_clip = args.grad_clip or training_config.grad_clip

        # Create datasets with the configured batch size
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,  # Use the resolved batch_size
            shuffle=True, 
            num_workers=1
        )

        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size,  # Use the resolved batch_size
            shuffle=False, 
            num_workers=1
        )

        # Enable gradient checkpointing if configured
        if training_config.gradient_checkpointing:
            model.gradient_checkpointing_enable()

        # Initialize mixed precision scaler if enabled
        scaler = GradScaler() if training_config.mixed_precision else None

        # Get memory efficient optimizer
        optimizer = get_optimizer(model, training_config)

        # Add cosine learning rate scheduler with warmup
        total_steps = len(train_loader) * args.epochs
        scheduler = get_scheduler(
            optimizer,
            total_steps=total_steps,
            warmup_steps=warmup_steps
        )

        # Use command line value if provided, otherwise use config value
        grad_accum_steps = (
            args.gradient_accumulation_steps or 
            training_config.gradient_accumulation_steps
        )

        # Adjust total steps for scheduler to account for gradient accumulation
        total_steps = (len(train_loader) // grad_accum_steps) * args.epochs
        scheduler = get_scheduler(
            optimizer,
            total_steps=total_steps,
            warmup_steps=warmup_steps
        )

        # Use command line value if provided, otherwise use config value
        save_interval = args.save_interval or training_config.save_interval

        # Get tokenizer for showing completions
        tokenizer = NewickTokenizer()

        # Training loop
        best_val_loss = float("inf")
        for epoch in range(args.epochs):
            train_loss = train_epoch(
                model,
                train_loader, 
                optimizer,
                scheduler,
                device,
                epoch,
                scaler=scaler,
                mixed_precision=training_config.mixed_precision,
                gradient_accumulation_steps=grad_accum_steps
            )
            logger.info(f"Epoch {epoch} - Train loss: {train_loss:.4f}")

            # Show example completions instead of validation metrics
            logger.info("Example completions:")
            show_example_completions(model, val_loader, device, tokenizer)

            # Save checkpoint if we've hit the save interval
            if save_interval > 0 and (epoch + 1) % save_interval == 0:
                checkpoint_path = os.path.join(args.save_dir, f"model_epoch_{epoch}.pt")
                torch.save(
                    {
                        "epoch": epoch,
                        "model_state_dict": model.state_dict(),
                        "optimizer_state_dict": optimizer.state_dict(),
                        "train_loss": train_loss,
                    },
                    checkpoint_path,
                )
                logger.info(f"Saved checkpoint to {checkpoint_path}")


if __name__ == "__main__":
    main()

================
File: utils.py
================
from data import InputPair
from smallperm import PseudoRandomPermutation as PRP
import treeswift as ts
from typing import List, Set, Dict
import typer
from pathlib import Path
import pandas as pd
import random

# One-time pad constant for seed modification
OTP: int = 1000000007

def random_projection(
    gene_trees: List[str],
    species_tree: str,
    target_dimension: int,
    num_gene_trees: int,
    seed: int,
) -> InputPair:
    """
    Performs random projection on phylogenetic trees by selecting a subset of taxa.
    
    Args:
        gene_trees: List of Newick strings representing gene trees
        species_tree: Newick string representing the species tree
        target_dimension: Number of taxa to keep in the projection
        num_gene_trees: Number of gene trees to sample
        seed: Random seed for reproducibility
    
    Returns:
        InputPair containing projected gene trees and species tree
    """
    # Sample subset of gene trees
    subsetting_prp = PRP(len(gene_trees), seed ^ OTP)
    sampled_trees = [gene_trees[i] for i in subsetting_prp[:num_gene_trees]]
    
    # Parse Newick strings into TreeSwift objects
    gene_tree_objects = [ts.read_tree_newick(tree) for tree in sampled_trees]
    species_tree_object = ts.read_tree_newick(species_tree)
    
    # Collect all taxa from species tree
    taxa: Set[str] = {leaf.label for leaf in species_tree_object.traverse_leaves()}
    
    # Select random subset of taxa
    taxa_sorted = sorted(list(taxa))
    prp = PRP(len(taxa), seed)
    selected_taxa = [taxa_sorted[i] for i in prp[:target_dimension]]
    taxa_to_index = {taxon: i for i, taxon in enumerate(selected_taxa)}
    
    # Extract subtrees with selected taxa
    projected_gene_trees = []
    for tree in gene_tree_objects:
        subtree = tree.extract_tree_with(selected_taxa)
        subtree.rename_nodes(taxa_to_index)
        subtree = only_topology(subtree)
        subtree = unroot(subtree)
        projected_gene_trees.append(subtree)
        
        
    projected_species_tree = species_tree_object.extract_tree_with(selected_taxa)
    projected_species_tree.rename_nodes(taxa_to_index)

    projected_species_tree = only_topology(projected_species_tree)
    return InputPair(projected_gene_trees, projected_species_tree)


def unroot(tree):
    """
    Unroots treeswift tree. Adapted from treeswift 'deroot' function.
    This one doesn't contract (A,B); to A;

    Parameters
    ----------
    tree: treeswift tree

    Returns unrooted treeswift tree
    """
    if tree.root == None:
        return tree
    if tree.root.num_children() == 2:
        [left, right] = tree.root.child_nodes()
        if not right.is_leaf():
            right.contract()
        elif not left.is_leaf():
            left.contract()
    tree.is_rooted = False
    return tree

def only_topology(
    tree: ts.Tree,
) -> ts.Tree:
    for node in tree.traverse_postorder():
        node.edge_length = None
    return tree

def process_directory(
    input_dir: Path,
    target_dimension: int,
    num_gene_trees: int,
    seed: int,
) -> Dict:
    """
    Process a single directory and return results as a dictionary.
    """
    species_tree_path = input_dir / "s_tree.trees"
    gene_trees_path = input_dir / "truegenetrees"
    
    # Read input files
    with open(species_tree_path) as f:
        species_tree = f.read().strip()
    
    with open(gene_trees_path) as f:
        gene_trees = [line.strip() for line in f if line.strip()]
    
    # Perform random projection
    projected = random_projection(
        gene_trees=gene_trees,
        species_tree=species_tree,
        target_dimension=target_dimension,
        num_gene_trees=num_gene_trees,
        seed=seed,
    )
    
    return {
        "gtrees": [tree.newick() for tree in projected.gtrees],
        "species_tree": projected.stree.newick().lstrip("[&R] "),
        "directory": str(input_dir),
        "seed": seed,
        "num_trees": num_gene_trees,
        "target_dim": target_dimension
    }

def main(
    base_dir: Path = typer.Argument(..., help="Base directory containing numbered subdirectories"),
    target_dimension: int = typer.Option(16, "--target-dim", "-d", help="Number of taxa to keep in projection"),
    num_iterations: int = typer.Option(1000, "--iterations", "-i", help="Number of iterations per directory"),
    output_file: Path = typer.Option("projected_data.parquet", "--output", "-o", help="Output parquet file"),
    start_dir: int = typer.Option(1, "--start", help="Starting directory number"),
    end_dir: int = typer.Option(10, "--end", help="Ending directory number"),
) -> None:
    """
    Process multiple directories and save results to a parquet file.
    Expects directories in format: {base_dir}/01, {base_dir}/02, etc.
    """
    all_results = []
    
    for dir_num in range(start_dir, end_dir + 1):
        input_dir = base_dir / f"{dir_num:02d}"
        if not input_dir.exists():
            print(f"Skipping non-existent directory: {input_dir}")
            continue
            
        print(f"Processing directory: {input_dir}")
        
        for i in range(num_iterations):
            seed = i
            num_gene_trees = random.randint(100, 200)
            
            result = process_directory(
                input_dir=input_dir,
                target_dimension=target_dimension,
                num_gene_trees=num_gene_trees,
                seed=seed,
            )
            all_results.append(result)
            
            if (i + 1) % 100 == 0:
                print(f"Completed {i + 1} iterations for {input_dir}")
    
    # Convert to DataFrame and save as parquet
    df = pd.DataFrame(all_results)
    df.to_parquet(output_file, compression='snappy')
    print(f"Results saved to {output_file}")

if __name__ == "__main__":
    typer.run(main)
